{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OjH_mEO1gFwb"
      },
      "outputs": [],
      "source": [
        "# Une erreur s'est produite lors de l'installation à l'aide de la commande PIP, je l'ai donc ajoutée.\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geh2kJNDfo3x",
        "outputId": "dd1fd6ef-8d7b-4826-ab64-a88bf0dc7f92"
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers torch accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9nLLvPufjBL"
      },
      "outputs": [],
      "source": [
        "# Charger le modèle directement\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig\n",
        "# Charger le tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "# Configuration du modèle\n",
        "config = AutoConfig.from_pretrained(\"google/flan-t5-base\", trust_remote_code=True)#, token=token)\n",
        "config.init_device = \"cuda\"\n",
        "config.temperature = 0.1\n",
        "config.max_length =300\n",
        "config.eos_token_id=tokenizer.eos_token_id\n",
        "config.pad_token_id=tokenizer.pad_token_id\n",
        "config.do_sample = True\n",
        "\n",
        "# Charger le modèle\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\",device_map=\"cuda\", config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIjCbi6Hf4yw",
        "outputId": "5364d787-5bfc-496e-b6b1-10b0ef20cfc2"
      },
      "outputs": [],
      "source": [
        "input_text = \"résumé ce texte : ChatGPT est un chatbot d'intelligence artificielle (IA) qui utilise le traitement du langage naturel pour créer un dialogue conversationnel semblable à celui d'un humain. Le modèle linguistique peut répondre aux questions et composer divers contenus écrits, notamment des articles, des publications sur les réseaux sociaux, des essais, du code et des e-mails. ChatGPT est une forme d'IA générative : un outil qui permet aux utilisateurs de saisir des invites pour recevoir des images, du texte ou des vidéos de type humain créés par l'IA. ChatGPT est similaire aux services de chat automatisés trouvés sur les sites Web de service client, car les gens peuvent lui poser des questions. ou demandez des éclaircissements sur les réponses de ChatGPT. Le GPT signifie Generative Pre-trained Transformer, qui fait référence à la façon dont ChatGPT traite les demandes et formule les réponses. ChatGPT est formé à l'apprentissage par renforcement grâce à des commentaires humains et à des modèles de récompense qui classent les meilleures réponses. Ces commentaires permettent d'augmenter ChatGPT avec l'apprentissage automatique pour améliorer les réponses futures.\"\n",
        "\n",
        "# Tokenizer le texte ci-dessus\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# Obtenir la sortie du modèle\n",
        "outputs = model.generate(input_ids)\n",
        "\n",
        "# Print la réponse\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC2aP_z9yPj6",
        "outputId": "eb8e7c2a-fe92-4544-914c-9dd9a3ea608f"
      },
      "outputs": [],
      "source": [
        "input_text = \"Répondez à la question suivante : Pouvez-vous me dire qui est le président de la France ?\"\n",
        "\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb-xKGVuhtka",
        "outputId": "b81c2d3d-46a2-4b02-f5fe-a58326fdc4f5"
      },
      "outputs": [],
      "source": [
        "# Appeler l'API Huggingface\n",
        "import requests\n",
        "# Déclarer des jetons\n",
        "API_TOKEN=\"Votre_CLÉ_API_Huggingface\"\n",
        "\n",
        "# Déclarer l'URL\n",
        "API_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-base\"\n",
        "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
        "\n",
        "# appeler l'API et obtenir les résultats\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "output = query({\n",
        "\t\"inputs\": \"J'aime manger du pain, par exemple\",\n",
        "})\n",
        "\n",
        "# In ra output\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ-VkOGWiZvO",
        "outputId": "2648b563-fca9-4c5a-d544-6616fc304129"
      },
      "outputs": [],
      "source": [
        "# Appeler l'API Huggingface\n",
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-large\"\n",
        "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
        "\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "output = query({\n",
        "\t\"inputs\": \"J'aime boire des boissons soft, comme\",\n",
        "})\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOS2bdMwk_OE",
        "outputId": "b44eb514-2658-4727-b1ae-a0fab5c38dff"
      },
      "outputs": [],
      "source": [
        "!pip install openai --upgrade\n",
        "!pip install typing-extensions --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-dVmXIptiKv2"
      },
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "from openai import OpenAI\n",
        "\n",
        "# Appeler openAI pour que nous puissions compléter la phrase\n",
        "client = OpenAI(\n",
        "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
        "    api_key=\"Votre_CLÉ_API\",\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Qui est le président de la République française ?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.5,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Vq0zFm2OkMh0",
        "outputId": "1683790a-da60-41d4-d635-a65d0034120f"
      },
      "outputs": [],
      "source": [
        "# fin de la conversation\n",
        "chat_completion.choices[0].message.content"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
